Related:

[[The Information Bottleneck Theory of Deep Learning]]
[[Deep Double Descent]]

Papers:
[Understanding Generalization through Visualizations](https://arxiv.org/abs/1906.03291)
[The Case for Bayesian Deep Learning](https://arxiv.org/abs/2001.10995)
[Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data](https://arxiv.org/abs/1703.11008) (an optimization theory take on why SGD might generalize)

[Triple descent and the two kinds of overfitting: Where & why do they appear?](https://arxiv.org/abs/2006.03509) (some theory on [[Deep Double Descent]])

[Every Model Learned by Gradient Descent Is Approximately a Kernel Machine](https://arxiv.org/abs/2012.00152)

