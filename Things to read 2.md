Try to understand how Bau's Trace/TransformerLens are implemented, so that hooks stop bothering you.
ARENA 3's explainer on hooks may help.

[Thinking Like Transformers](https://arxiv.org/pdf/2106.06981) and [Tracr](https://arxiv.org/pdf/2301.05062), which compiled their language


https://www.lesswrong.com/posts/JeuAk53QfWauaGWfS/instruction-tuning-and-autoregressive-distribution-shift

[Glitch Tokens](https://arxiv.org/pdf/2404.09894)

[Instruction Tuning for Large Language Models: A Survey](https://arxiv.org/abs/2308.10792)

[Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector)

[From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step](https://arxiv.org/pdf/2405.14838)
- Is this a good idea?
- It invests training time compute into less test time compute, destroys some interpretability properties
- Promised result is really strong, though
### Other facts I learned today
- GPT-3 was shut down; there is now no way to know what GPT-3 would say to a given prompt.


