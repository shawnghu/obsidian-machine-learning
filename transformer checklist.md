
- find a dataset
- train dataset against preset implementation (probably pytorch's, now)
	- how do i do the word embeddings?
	- do i have to do the sequence embeddings myself?
		- how does sequence embedding for a gpt work?
	- check to see that it works:
		- training loss goes down
		- overfit to small dataset
		- 
- figure out what the difference between single head and multi head attention are
- reimplement the attention part
- 